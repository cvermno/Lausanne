{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "TXdq7Qp8I1Ja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZN3b4m0YJLd"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import os\n",
        "os.system('pip install spacy')\n",
        "os.system('python -m spacy download fr_core_news_sm')\n",
        "\n",
        "import spacy\n",
        "import string\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "import gensim.downloader as api\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discovering the CamemBERT model"
      ],
      "metadata": {
        "id": "_OWrN8EZNFWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facing the reality that classic ML models were limited in achieving a high accuracy (maximum accuracy so far = 0.46), we embarked on a quest for innovative solutions. In our pursuit, we encountered the CamemBERT model, a cutting-edge neural network architecture tailored for natural language understanding tasks.\n",
        "\n",
        "How does it work? First, CamemBERT undergoes a pre-training phase where it familiarizes itself with the nuances of the French language by digesting vast amounts of text data. During this stage, it learns to comprehend relationships between words and sentences, leveraging a technique called self-attention to capture contextual dependencies effectively. Once pre-training is complete, CamemBERT can be fine-tuned for specific tasks, such as predicting the difficulty of French sentences."
      ],
      "metadata": {
        "id": "SiG10UqBfjKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on 80% of training data, test on 20% of training data"
      ],
      "metadata": {
        "id": "T4wYETwsBayn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "training = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/training_data.csv\")\n",
        "\n",
        "# Split your data into features (X) and target variable (y)\n",
        "X = training['sentence']\n",
        "y = training['difficulty']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 123)\n",
        "\n",
        "# Encode labels: convert difficulty levels (A1, A2, etc.) into numerical labels.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define a custom dataset class: this tokenizes the input sentences using the CamemBERT tokenizer and prepares them for input to the model.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),  # Assuming labels are already numerical\n",
        "        }\n",
        "\n",
        "# Define the model's parameters\n",
        "MAX_LEN = 310\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize the CamemBERT tokenizer and the CamemBERT model with the base pre-trained weights\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = CustomDataset(X_train.values, y_train_encoded, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(X_test.values, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "# Create data loaders to efficiently feed batches of data to the model during training and evaluation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch} of {NUM_EPOCHS}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    k = 1\n",
        "    for batch in train_loader:\n",
        "        print(f\"\\tBatch {k} of {len(train_loader)}\")\n",
        "        k += 1\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_decoded = label_encoder.inverse_transform(test_labels)\n",
        "\n",
        "# Create a DataFrame with sentences and corresponding difficulty predictions\n",
        "output_data = pd.DataFrame({\"id\": X_test.index, \"difficulty\": y_test_decoded})\n",
        "output_data"
      ],
      "metadata": {
        "id": "FGkZCkXkBZ9W",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways: The test accuracy (0.5229) notably indicates high performance, while the loss (6.1162) remains relatively low, suggesting effective model training. The model shows great potential!"
      ],
      "metadata": {
        "id": "UaJVBtBLZkC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on 100% training data, test on the unlabelled test data"
      ],
      "metadata": {
        "id": "3LTZwFVHBRxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and test data\n",
        "training = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/training_data.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/unlabelled_test_data.csv\")\n",
        "\n",
        "# Split your train data into features (X) and target variable (y)\n",
        "X = training['sentence']\n",
        "y = training['difficulty']\n",
        "\n",
        "# Split your test data into features (X) and target variable (y)\n",
        "X_final = test['sentence']\n",
        "\n",
        "# Encode labels: convert difficulty levels (A1, A2, etc.) into numerical labels.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define a custom dataset class: this tokenizes the input sentences using the CamemBERT tokenizer and prepares them for input to the model.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),  # Assuming labels are already numerical\n",
        "        }\n",
        "\n",
        "# Define the model's parameters\n",
        "MAX_LEN = 310\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize the CamemBERT tokenizer and the CamemBERT model with the base pre-trained weights\n",
        "tokenizer_final = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model_final = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = CustomDataset(X.values, y_encoded, tokenizer_final, MAX_LEN)\n",
        "test_dataset = CustomDataset(X_final.values, [0]*len(X_final), tokenizer_final, MAX_LEN)\n",
        "\n",
        "# Create data loaders to efficiently feed batches of data to the model during training and evaluation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model_final.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_final.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch} of {NUM_EPOCHS}\")\n",
        "    model_final.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    k = 1\n",
        "    for batch in train_loader:\n",
        "        print(f\"\\tBatch {k} of {len(train_loader)}\")\n",
        "        k += 1\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_final(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test loop\n",
        "model_final.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_final(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        # Get predicted labels\n",
        "        predicted_labels = np.argmax(logits, axis=1)\n",
        "        test_predictions.extend(predicted_labels)\n",
        "\n",
        "# Convert predicted labels back to original labels\n",
        "predicted_labels = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "test['predicted_difficulty'] = predicted_labels\n",
        "test.drop(columns=['sentence'], inplace=True)\n",
        "test.rename(columns={'predicted_difficulty': 'difficulty'}, inplace=True)\n",
        "test.to_csv('final.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "files.download(\"final.csv\")"
      ],
      "metadata": {
        "id": "zSrOjzMF0LO4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning the model: adjusting the parameters"
      ],
      "metadata": {
        "id": "WL1auPKbhIZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning the camemBERT model involves adjusting its parameters to minimize the disparity between predicted and actual difficulty levels. Let's take a closer look at the definition and significance of each parameter, understanding its role within the model:\n",
        "\n",
        "1. **MAX_LEN**: This is the maximum number of words the model looks at in a sentence. Sequences longer than this length will be truncated, while shorter sequences will be padded to match this length. Given that the maximum number of words in our dataset sentences is 304, we opt to set the maximum length to 310. This choice aims to preserve all valuable information within sentences, while maintaining computational efficiency.\n",
        "2. **BATCH_SIZE**: This is the number of sentences processed simultaneously by the model during each iteration of training or evaluation. Larger batch sizes typically result in faster training but may require more memory. Conversely, smaller batch sizes may lead to slower training but can sometimes yield better generalization. A common practice is to use a batch size that divides the total number of samples evenly. With 1200 sentences in our test set, a batch size of 32 would be suitable, providing approximately 37 batches for the entire dataset. We validated this choice empirically by experimenting with different BATCH_SIZE values.\n",
        "3. **LEARNING_RATE**: This is the step size (gradient descent) taken during optimization to update the model's parameters. A higher learning rate allows for faster convergence but may lead to overshooting the optimal solution or instability in training. On the other hand, a lower learning rate may result in slower convergence but can yield more stable training and better performance. A commonly recommended starting point is to use a learning rate within the range of 1e-5 to 5e-5 for fine-tuning CamemBERT models.\n",
        "4. **NUM_EPOCHS**: This is how many times the model looks at the entire dataset. Training for more epochs allows the model to learn from the data multiple times, potentially improving its performance. However, training for too many epochs can lead to overfitting, where the model memorizes the training data and performs poorly on unseen data. Starting with a relatively small number of epochs, such as 10, is a common practice in model training."
      ],
      "metadata": {
        "id": "EPoCOqVMhPGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a summary table presenting the accuracy levels achieved for various parameter configurations. This table was generated by importing the predictions dataframe for the model trained on 100% of the training data to Kaggle."
      ],
      "metadata": {
        "id": "nmqEsswVqbhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_optimization = [\n",
        "    [\"MAX_LEN\", \"BATCH_SIZE\", \"LEARNING_RATE\", \"NUM_EPOCHS\", \"Accuracy\"],\n",
        "    [310, 16, \"2e-5\", 10, 0.581],\n",
        "    [310, 16, \"2e-5\", 20, 0.555],\n",
        "    [310, 16, \"1e-5\", 10, 0.557],\n",
        "    [310, 16, \"1e-5\", 20, 0.540],\n",
        "    [310, 32, \"2e-5\", 10, 0.583],\n",
        "    [310, 32, \"2e-5\", 20, 0.521],\n",
        "    [310, 32, \"1e-5\", 10, 0.551],\n",
        "    [310, 32, \"1e-5\", 20, 0.518],\n",
        "    [310, 16, \"2e-5\", 5, 0.571],\n",
        "    [310, 32, \"2e-5\", 5, 0.554],\n",
        "    [310, 16, \"10e-5\", 20, 0.155]\n",
        "]\n",
        "\n",
        "parameters_optimization_df = pd.DataFrame(parameters_optimization[1:], columns=parameters_optimization[0])\n",
        "parameters_optimization_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "5iJr9B6QnsRo",
        "outputId": "a3a95438-f378-4bbb-bdf9-7c43b16cd458"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    MAX_LEN  BATCH_SIZE LEARNING_RATE  NUM_EPOCHS  Accuracy\n",
              "0       310          16          2e-5          10     0.581\n",
              "1       310          16          2e-5          20     0.555\n",
              "2       310          16          1e-5          10     0.557\n",
              "3       310          16          1e-5          20     0.540\n",
              "4       310          32          2e-5          10     0.583\n",
              "5       310          32          2e-5          20     0.521\n",
              "6       310          32          1e-5          10     0.551\n",
              "7       310          32          1e-5          20     0.518\n",
              "8       310          16          2e-5           5     0.571\n",
              "9       310          32          2e-5           5     0.554\n",
              "10      310          16         10e-5          20     0.155"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eda1a072-31c6-472d-940c-9b0bdbfb1215\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAX_LEN</th>\n",
              "      <th>BATCH_SIZE</th>\n",
              "      <th>LEARNING_RATE</th>\n",
              "      <th>NUM_EPOCHS</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>1e-5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>1e-5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>310</td>\n",
              "      <td>32</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>310</td>\n",
              "      <td>32</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>310</td>\n",
              "      <td>32</td>\n",
              "      <td>1e-5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>310</td>\n",
              "      <td>32</td>\n",
              "      <td>1e-5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>310</td>\n",
              "      <td>32</td>\n",
              "      <td>2e-5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>310</td>\n",
              "      <td>16</td>\n",
              "      <td>10e-5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda1a072-31c6-472d-940c-9b0bdbfb1215')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eda1a072-31c6-472d-940c-9b0bdbfb1215 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eda1a072-31c6-472d-940c-9b0bdbfb1215');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-022d51e1-6af0-44fb-855c-0ed94cde6311\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-022d51e1-6af0-44fb-855c-0ed94cde6311')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-022d51e1-6af0-44fb-855c-0ed94cde6311 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "parameters_optimization_df",
              "summary": "{\n  \"name\": \"parameters_optimization_df\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"MAX_LEN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 310,\n        \"max\": 310,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          310\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BATCH_SIZE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 16,\n        \"max\": 32,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LEARNING_RATE\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2e-5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NUM_EPOCHS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 5,\n        \"max\": 20,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12209430036581634,\n        \"min\": 0.155,\n        \"max\": 0.587,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.521\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring further text processing techniques: lemmatization"
      ],
      "metadata": {
        "id": "bWbHoxNCGLfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing words to their base or root form. Here, it is applied to the text data before tokenization. This ensures that the tokenized text used for training and inference contains standardized representations of words, potentially improving model performance and interpretability. Let's investigate its efficacy."
      ],
      "metadata": {
        "id": "3WsL62Gcwxeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN ON 80% OF THE TRAINING DATA, TEST ON 20% OF THE TRAINING DATA\n",
        "\n",
        "# Load the training data\n",
        "training = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/training_data.csv\")\n",
        "\n",
        "# Create tokenizer function for preprocessing\n",
        "def spacy_tokenizer(text):\n",
        "\n",
        "    # Define stopwords, punctuation, and numbers\n",
        "    stop_words = stopwords.words('french')\n",
        "    punctuations = string.punctuation +'–' + '—'\n",
        "    numbers = \"0123456789\"\n",
        "\n",
        "    # Create spacy object\n",
        "    mytokens = nlp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = ([ word.lemma_.lower().strip() for word in mytokens ])\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = ([ word for word in mytokens\n",
        "                 if word not in stop_words and word not in punctuations ])\n",
        "\n",
        "    # Remove sufix like \".[1\" in \"experience.[1\"\n",
        "    mytokens_2 = []\n",
        "    for word in mytokens:\n",
        "        for char in word:\n",
        "            if (char in punctuations) or (char in numbers):\n",
        "                word = word.replace(char, \"\")\n",
        "        if word != \"\":\n",
        "            mytokens_2.append(word)\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens_2\n",
        "\n",
        "# Tokenize texts in training data\n",
        "training['processed_sentence'] = training['sentence'].apply(spacy_tokenizer)\n",
        "training['processed_sentence'] = training['processed_sentence'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Split your train data into features (X) and target variable (y)\n",
        "X = training['processed_sentence']\n",
        "y = training['difficulty']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 123)\n",
        "\n",
        "# Encode labels: convert difficulty levels (A1, A2, etc.) into numerical labels.from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define a custom dataset class: this tokenizes the input sentences using the CamemBERT tokenizer and prepares them for input to the model.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),  # Assuming labels are already numerical\n",
        "        }\n",
        "\n",
        "# Define the model's parameters\n",
        "MAX_LEN = 310\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize the CamemBERT tokenizer and the CamemBERT model with the base pre-trained weights\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = CustomDataset(X_train.values, y_train_encoded, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(X_test.values, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "# Create data loaders to efficiently feed batches of data to the model during training and evaluation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch} of {NUM_EPOCHS}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    k = 1\n",
        "    for batch in train_loader:\n",
        "        print(f\"\\tBatch {k} of {len(train_loader)}\")\n",
        "        k += 1\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_decoded = label_encoder.inverse_transform(test_labels)\n",
        "\n",
        "# Create a DataFrame with sentences and corresponding difficulty predictions\n",
        "output_data = pd.DataFrame({\"id\": X_test.index, \"difficulty\": y_test_decoded})\n",
        "output_data"
      ],
      "metadata": {
        "id": "R-TqxDNzEY50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways: After running this model with various parameters, we observed that the accuracy level did not surpass that of the previous model (0.4729 < 0.5229). Additionally, the loss more than doubled (14.7011 > 6.1162), indicating a less effective performance compared to the previous configuration."
      ],
      "metadata": {
        "id": "3aG0vXm2xzNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating data augmentation methods"
      ],
      "metadata": {
        "id": "jricjznyGpeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation by synonym replacement is a technique used to increase the diversity of training data. In this technique, words in the text are replaced with their synonyms while preserving the overall meaning of the text. The goal of this approach is to improve the robustness and generalization ability of machine learning models trained on that data.\n",
        "\n",
        "We will explore two different approaches: using word embeddings or leveraging pre-existing libraries. Let's delve into each of these approaches sequentially."
      ],
      "metadata": {
        "id": "VtstJiGrV78x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching training data using word embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5m7YOCiXTxfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What exactly is word embedding? It's a technique to represent words as vectors in a high-dimensional space. These vectors capture semantic relationships between words, meaning that similar words are represented by similar vectors.\n",
        "\n",
        "Within this data augmentation approach, words are replaced with similar ones based on their embeddings. Hence the model may learn more robust representations of language patterns and semantics."
      ],
      "metadata": {
        "id": "bdwe2bFAD0Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN ON 80% OF THE TRAINING DATA, TEST ON 20% OF THE TRAINING DATA\n",
        "\n",
        "# Load the training data\n",
        "training = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/training_data.csv\")\n",
        "\n",
        "# Split your data into features (X) and target variable (y)\n",
        "X = training['sentence']\n",
        "y = training['difficulty']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 123)\n",
        "\n",
        "# Encode labels: convert difficulty levels (A1, A2, etc.) into numerical labels.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Load pre-trained French word embeddings\n",
        "french_word_vectors = api.load(\"word2vec-ruscorpora-300\")\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len, augment=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.augment = augment\n",
        "\n",
        "    def word_embedding_augmentation(self, text):\n",
        "        augmented_text = []\n",
        "        for word in text.split():\n",
        "            if word in french_word_vectors:\n",
        "                similar_words = french_word_vectors.most_similar(word, topn=5)\n",
        "                augmented_text.append(similar_words[0][0])  # Choose the most similar word\n",
        "            else:\n",
        "                augmented_text.append(word)\n",
        "        return ' '.join(augmented_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            text = self.word_embedding_augmentation(text)\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),  # Assuming labels are already numerical\n",
        "        }\n",
        "\n",
        "# Define the model's parameters\n",
        "MAX_LEN = 310\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize the CamemBERT tokenizer and the CamemBERT model with the base pre-trained weights\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = CustomDataset(X_train.values, y_train_encoded, tokenizer, MAX_LEN, augment=True)\n",
        "test_dataset = CustomDataset(X_test.values, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "# Create data loaders to efficiently feed batches of data to the model during training and evaluation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch} of {NUM_EPOCHS}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    k = 1\n",
        "    for batch in train_loader:\n",
        "        print(f\"\\tBatch {k} of {len(train_loader)}\")\n",
        "        k += 1\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_decoded = label_encoder.inverse_transform(test_labels)\n",
        "\n",
        "# Create a DataFrame with sentences and corresponding difficulty predictions\n",
        "output_data = pd.DataFrame({\"id\": X_test.index, \"difficulty\": y_test_decoded})\n",
        "output_data"
      ],
      "metadata": {
        "id": "l1gU2uqkDxV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways: Utilizing word embeddings to replace words with synonyms results in a  higher accuracy (0.5802 > 0.5229). However, there's a slight increase in loss (7.8193 > 6.1162), although it remains relatively consistent."
      ],
      "metadata": {
        "id": "nRrQi85KDyps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching training data using synonyms from NLTK's WordNet"
      ],
      "metadata": {
        "id": "Kwm3CeACzcU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this method, words within sentences are randomly substituted with their synonyms retrieved from NLTK's WordNet, thereby enhancing the diversity of the training data by generating new variations of the original sentences."
      ],
      "metadata": {
        "id": "AMS5Als5MUX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN ON 80% OF THE TRAINING DATA, TEST ON 20% OF THE TRAINING DATA\n",
        "\n",
        "# Load the training data\n",
        "training = pd.read_csv(\"https://raw.githubusercontent.com/cvermno/ML-Project/main/Datasets/training_data.csv\")\n",
        "\n",
        "# Split your data into features (X) and target variable (y)\n",
        "X = training['sentence']\n",
        "y = training['difficulty']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 123)\n",
        "\n",
        "# Encode labels: convert difficulty levels (A1, A2, etc.) into numerical labels.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Function to get synonyms of a word\n",
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def synonym_replacement(sentence, n):\n",
        "    words = word_tokenize(sentence)\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if word not in stopwords.words('french')]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    sentence = ' '.join(new_words)\n",
        "    return sentence\n",
        "\n",
        "# Define a custom dataset class: this tokenizes the input sentences using the CamemBERT tokenizer and prepares them for input to the model.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len, augment=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "           text = synonym_replacement(text, 1)  # You can adjust the number of replacements\n",
        "\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),  # Assuming labels are already numerical\n",
        "        }\n",
        "\n",
        "# Define the model's parameters\n",
        "MAX_LEN = 310\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize the CamemBERT tokenizer and the CamemBERT model with the base pre-trained weights\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = CustomDataset(X_train.values, y_train_encoded, tokenizer, MAX_LEN, augment=True)\n",
        "test_dataset = CustomDataset(X_test.values, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "# Create data loaders to efficiently feed batches of data to the model during training and evaluation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch} of {NUM_EPOCHS}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    k = 1\n",
        "    for batch in train_loader:\n",
        "        print(f\"\\tBatch {k} of {len(train_loader)}\")\n",
        "        k += 1\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_decoded = label_encoder.inverse_transform(test_labels)\n",
        "\n",
        "# Create a DataFrame with sentences and corresponding difficulty predictions\n",
        "output_data = pd.DataFrame({\"id\": X_test.index, \"difficulty\": y_test_decoded})\n",
        "output_data"
      ],
      "metadata": {
        "id": "XVh6jQxmzk0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways: In contrast to the previous model, there's a reduction in accuracy (0.5604 < 0.5802), alongside an increase in loss (8.7431 > 7.8193). Consequently, this model fails to demonstrate any improvement in performance."
      ],
      "metadata": {
        "id": "skjto5iEJVtg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
